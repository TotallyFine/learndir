## 胶囊网络

### CNN缺点
CNN中接近输入的层学习检测诸如边缘图像像素中的重要特征，更高层将简单特征组合成更复杂的特征。更高级别的特征以加权和的方式将较低特征组合：卷积、激活。但是没有任何地方体现在构成更高级特征的简单特征之间存在姿态（平移、旋转）关系，CNN解决这个问题的方法是使用最大池化或连续卷积层来减少流经网络的数据的空间大小，从而增加高层神经元的“视野”，从而允许他们检测输入图像的较大区域的高阶特征。

但实际上最大池化会严重丢失信息，如果使用CNN仍不能解决问题，因为卷积神经网络内部数据表示不会考虑简单和复杂对象之间的空间位置信息。

动态路由相当于代替最大池化，将数据传输下去，提升了神经网络对数据的视野，并且没有丢失姿态信息（相对位置）。

对于空间内的信息（3D信息），可以认为大脑将从眼睛接受到视觉信息，然后解构看到的世界的层次表征，并试图将其与已经学习的模式和存储在大脑中的关系相匹配，这就是大脑识别的过程，关键在于大脑中的物体表示不依赖角度。

### 胶囊
像基本的神经元一样，他们也代表了一个认知思想的符号数字化。大脑的高层做了更多的演绎、理解和高层次的特征计算，大脑的特定部分在他们处理领域或主题上有明确的含义。在大脑中，给大脑的高层部分喂食较低级的特征，供大脑高层处理，从而将认知负荷从较高级别的处理中移除。如果较低级别的功能与大脑某些较高级别的部分不相关，则不应该发送到那里，不相关的信号应该被减弱。

如果用CNN来对一只狗进行识别，但因为所有的信息都向高层的传输，所以高层网络也在学习这个狗所在的背景中的情况，如果将输入的图像换成一个完全没有被训练的场景或者姿态或者外表，那么很可能出错。所以使用传统的CNN只能输入大量可能会用到的数据，才能弥补这个缺点。

胶囊被设想用来处理识别姿势问题，就是说，对于识别狗的问题，更高级别的部分用来处理复杂特征的识别和姿势认证，而较低的部分用来处理子特征。某个胶囊用来处理某个特征，比如一个胶囊识别耳朵，另一个识别鼻子。传统的CNN将特征分布到多个神经中，这样每个神经承受的压力很大。

每个胶囊是一组神经元，其输入输出向量表示特定实体类型的实例化参数（即特定物体、概念实体等出现的概率与某些属性）。使用输入输出向量的长度表征实体存在的概率，向量的方向表示实例化参数（即实体某些图像属性）。

### 动态路由
按照论文中的思想，找到最好的处理路径就等价与正确处理了图像，所以在Capsule中加入动态路由机制就可以找到一组系数C_ij，他们能另预测向量u最符合输出向量v，C_ij是耦合系数（coupling coefficients），该系数由动态路由过程迭代更新与确定。Capsule i与后一层所有的Capsule之间的耦合系数和为1,即C_11 C_12 ... C_1n相加为1,那么C_21 C_22 ... C_2n相加也为1。

耦合系数由routing softmax决定，且softmax函数中的logits b_ij初始化为0,耦合系数C_ij的softmax计算方式为
`C_ij = exp(b_ij) / ∑(k=i, n) exp(b_ik)`
更新`b_ij <- b_ij + u_ij' * Vj` u_ij' 是加权后的u。

对于所有在 l 层的 Capsule i 和在 l+1 层的 Capsule j，先初始化 b_ij 等于零。然后迭代 r 次，每次先根据 b_i 计算 c_i，然后在利用 c_ij 与 u_j|i' 计算 s_j 与 v_j。利用计算出来的 v_j 更新 b_ij 以进入下一个迭代循环更新 c_ij。该 Routing 算法十分容易收敛，基本上通过 3 次迭代就能有不错的效果。

### 损失函数与最优化
耦合系数是通过动态路由算法更新的，不需要反向传播。网络中其他卷积参数和Capsule中的权重都需要损失函数进行反向传播，原文中使用了Margin Loss
`Lc = Tc * max(0, m+ - ||Vc||)^2 + lamda*(1-Tc)*max(0, ||Vc||-m-)^2`
c是分类的类别，如果图片中存在这个类别则Tc为1否则为0,m+ 为上边界 m-为下边界，||Vc||是L2距离 欧式距离。

因为实例化向量的长度来表示 Capsule 要表征的实体是否存在，所以当且仅当图片里出现属于类别 k 的手写数字时，我们希望类别 k 的最顶层 Capsule 的输出向量长度很大（在本论文 CapsNet 中为 DigitCaps 层的输出）。为了允许一张图里有多个数字，我们对每一个表征数字 k 的 Capsule 分别给出单独的 Margin loss。
